{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple interface for helmet on head detection using trained model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предварительная установка пакетов (для Colab)\n",
    "#!pip install torchinfo ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.image as img\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "#import cv2\n",
    "import os\n",
    "import json\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "#import typing\n",
    "#import xml.etree.ElementTree as ET\n",
    "#import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.io import read_image\n",
    "from torchvision.ops import box_convert\n",
    "from torchinfo import summary\n",
    "from torchvision.io.image import read_image\n",
    "from torchvision.models.detection import ssdlite320_mobilenet_v3_large, SSDLite320_MobileNet_V3_Large_Weights, fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.mobilenetv3 import mobilenet_v3_large, MobileNet_V3_Large_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image, pil_to_tensor\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms._presets import ObjectDetection\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import smooth_l1_loss, cross_entropy\n",
    "#?\n",
    "from torchvision.models.detection.ssdlite import SSDLiteClassificationHead\n",
    "#from ultralytics import YOLO\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose place and set default path to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Устанавливаем путь до датасета\n",
    "\n",
    "# Локально\n",
    "#ds_path = '/home/vovk/SberUniversity/DS_from_03_10_2022/PM_Group_AN/FinalProj/HelmetDetection/VOC2028/'\n",
    "# relative path\n",
    "ds_path = 'VOC2028/'\n",
    "\n",
    "# Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# ds_path = '/content/drive/MyDrive/SberUniversity/PM_DS14AN/Fin/HelmetDetection/VOC2028/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepairing **SSDLite** model for prediction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create template for loading model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = ssdlite320_mobilenet_v3_large( weights='DEFAULT',\n",
    "                                      weights_backbone=MobileNet_V3_Large_Weights.IMAGENET1K_V1,\n",
    "                                      score_tresh=0.25\n",
    "                                     )\n",
    "\n",
    "m_3class = ssdlite320_mobilenet_v3_large( num_classes=3,\n",
    "                                          weights_backbone=None,\n",
    "                                          score_tresh=0.25,\n",
    "                                        )\n",
    "\n",
    "model_loaded.head = m_3class.head"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained 49 epoches\n",
      "Boxes loss: 1.5395, class loss: 1.9102\n"
     ]
    }
   ],
   "source": [
    "#best_path = ds_path + 'tmp/models/' + 'SSDLiteMobNetFreezBackbone_3class_best.pt'\n",
    "#best_path = ds_path + 'tmp/models/' + 'SSDLiteMobNetFreezBackbone_3class_01_best(34ep).pt'\n",
    "best_path = ds_path + 'tmp/models/' + 'SSDLiteMobNetFreezBackbone_3class_best(49ep).pt'\n",
    "checkpoint = torch.load(best_path, map_location=torch.device('cpu'))\n",
    "model_loaded.load_state_dict(checkpoint['model_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "print('Model trained', epoch, 'epoches')\n",
    "print(f'Boxes loss: {round(loss[\"bbox_loss\"].item(),4)}, class loss: {round(loss[\"cls_loss\"].item(),4)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction using trained **SSDLite** model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for Colab\n",
    "#from google.colab import files\n",
    "#img1 = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boxes': tensor([[364.4760, 115.8751, 416.0000, 191.6312]], grad_fn=<StackBackward0>), 'scores': tensor([0.7539], grad_fn=<IndexBackward0>), 'labels': tensor([2])}\n"
     ]
    }
   ],
   "source": [
    "img1 = Image.open(ds_path + 'short_test/'+ \"hard_hat_workers966.png\")\n",
    "convert_to_tensor = ObjectDetection() # Универсальное преобразование (используется в исходниках ssdlite)\n",
    "tensor_img1 = convert_to_tensor(img1)\n",
    "model_loaded.eval()\n",
    "model_loaded.score_thresh = 0.5\n",
    "prediction = model_loaded([tensor_img1])[0]\n",
    "\n",
    "labels_dict={'1': 'head',\n",
    "             '2': 'helmet'}\n",
    "\n",
    "labels = [labels_dict[str(label.item())] + ': ' + \\\n",
    "          str(round(prediction[\"scores\"][idx].item(), 2)) \\\n",
    "          for idx, label in enumerate(prediction[\"labels\"])]\n",
    "\n",
    "box = draw_bounding_boxes(pil_to_tensor(img1), # for original image case\n",
    "                          #(tensor_img1*256).to(dtype=torch.uint8), # for normalized image case (not fully identical)\n",
    "                          boxes=prediction['boxes'],\n",
    "                          labels=labels,\n",
    "                          colors='red',\n",
    "                          width=3)\n",
    "im = to_pil_image(box.detach())\n",
    "im.show()\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "002.jpg {'boxes': tensor([[361.3578,  68.3813, 450.0000, 184.5416],\n",
      "        [243.9342,  44.5466, 341.5432, 167.8883]], grad_fn=<StackBackward0>), 'scores': tensor([0.9392, 0.8468], grad_fn=<IndexBackward0>), 'labels': tensor([2, 2])}\n",
      "000009.jpg {'boxes': tensor([[243.1186,  56.5447, 358.3921, 199.8705],\n",
      "        [ 94.2763, 315.8340, 231.3785, 433.4378],\n",
      "        [367.1082,  30.9294, 465.0062, 146.2465],\n",
      "        [ 60.1443, 121.0476, 142.0041, 219.4489],\n",
      "        [381.2317,  36.9458, 470.8235, 149.4275]], grad_fn=<StackBackward0>), 'scores': tensor([0.9970, 0.7657, 0.7337, 0.6585, 0.5363], grad_fn=<IndexBackward0>), 'labels': tensor([2, 2, 2, 1, 1])}\n",
      "001_crop.jpg {'boxes': tensor([[176.0350, 405.4665, 333.5681, 546.3819]], grad_fn=<StackBackward0>), 'scores': tensor([0.5629], grad_fn=<IndexBackward0>), 'labels': tensor([2])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vovk/anaconda3/envs/ds/lib/python3.10/site-packages/torchvision/utils.py:210: UserWarning: boxes doesn't contain any box. No box was drawn\n",
      "  warnings.warn(\"boxes doesn't contain any box. No box was drawn\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am3_9_frame109.jpg {'boxes': tensor([], size=(0, 4), grad_fn=<StackBackward0>), 'scores': tensor([], grad_fn=<IndexBackward0>), 'labels': tensor([], dtype=torch.int64)}\n",
      "hard_hat_workers846.png {'boxes': tensor([[177.2856, 142.2804, 249.9699, 211.1718]], grad_fn=<StackBackward0>), 'scores': tensor([0.4275], grad_fn=<IndexBackward0>), 'labels': tensor([1])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vovk/anaconda3/envs/ds/lib/python3.10/site-packages/torchvision/utils.py:210: UserWarning: boxes doesn't contain any box. No box was drawn\n",
      "  warnings.warn(\"boxes doesn't contain any box. No box was drawn\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hard_hat_workers15.png {'boxes': tensor([], size=(0, 4), grad_fn=<StackBackward0>), 'scores': tensor([], grad_fn=<IndexBackward0>), 'labels': tensor([], dtype=torch.int64)}\n",
      "hard_hat_workers966.png {'boxes': tensor([[360.8781, 115.7856, 416.0000, 199.3054]], grad_fn=<StackBackward0>), 'scores': tensor([0.4306], grad_fn=<IndexBackward0>), 'labels': tensor([2])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vovk/anaconda3/envs/ds/lib/python3.10/site-packages/torchvision/utils.py:210: UserWarning: boxes doesn't contain any box. No box was drawn\n",
      "  warnings.warn(\"boxes doesn't contain any box. No box was drawn\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am3_9_frame111.jpg {'boxes': tensor([], size=(0, 4), grad_fn=<StackBackward0>), 'scores': tensor([], grad_fn=<IndexBackward0>), 'labels': tensor([], dtype=torch.int64)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vovk/anaconda3/envs/ds/lib/python3.10/site-packages/torchvision/utils.py:210: UserWarning: boxes doesn't contain any box. No box was drawn\n",
      "  warnings.warn(\"boxes doesn't contain any box. No box was drawn\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001.jpg {'boxes': tensor([], size=(0, 4), grad_fn=<StackBackward0>), 'scores': tensor([], grad_fn=<IndexBackward0>), 'labels': tensor([], dtype=torch.int64)}\n",
      "000012.jpg {'boxes': tensor([[0.0000e+00, 1.6722e+02, 2.8219e+02, 4.5515e+02],\n",
      "        [3.3546e+02, 1.2223e+02, 5.0028e+02, 3.4359e+02],\n",
      "        [5.2386e+02, 5.7271e-01, 7.9222e+02, 3.1288e+02]],\n",
      "       grad_fn=<StackBackward0>), 'scores': tensor([0.9823, 0.9237, 0.9002], grad_fn=<IndexBackward0>), 'labels': tensor([2, 2, 2])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vovk/anaconda3/envs/ds/lib/python3.10/site-packages/torchvision/utils.py:210: UserWarning: boxes doesn't contain any box. No box was drawn\n",
      "  warnings.warn(\"boxes doesn't contain any box. No box was drawn\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hard_hat_workers564.png {'boxes': tensor([], size=(0, 4), grad_fn=<StackBackward0>), 'scores': tensor([], grad_fn=<IndexBackward0>), 'labels': tensor([], dtype=torch.int64)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vovk/anaconda3/envs/ds/lib/python3.10/site-packages/torchvision/utils.py:210: UserWarning: boxes doesn't contain any box. No box was drawn\n",
      "  warnings.warn(\"boxes doesn't contain any box. No box was drawn\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am3_9_frame111_crop.jpg {'boxes': tensor([], size=(0, 4), grad_fn=<StackBackward0>), 'scores': tensor([], grad_fn=<IndexBackward0>), 'labels': tensor([], dtype=torch.int64)}\n",
      "am3_9_frame109_crop.jpg {'boxes': tensor([[181.8925, 198.8029, 240.8736, 290.4316]], grad_fn=<StackBackward0>), 'scores': tensor([0.4727], grad_fn=<IndexBackward0>), 'labels': tensor([2])}\n"
     ]
    }
   ],
   "source": [
    "convert_to_tensor = ObjectDetection() # Универсальное преобразование (используется в исходниках ssdlite)\n",
    "model_loaded.eval()\n",
    "model_loaded.score_thresh = 0.4\n",
    "\n",
    "labels_dict={'1': 'head',\n",
    "             '2': 'helmet'}\n",
    "colors_dict={'1': 'red',\n",
    "             '2': 'green'}\n",
    "\n",
    "test_path = ds_path + 'short_test/'\n",
    "img_lst = os.listdir(test_path)\n",
    "\n",
    "for img_name in img_lst:\n",
    "    img1 = Image.open(test_path + img_name)\n",
    "    \n",
    "    tensor_img1 = convert_to_tensor(img1)\n",
    "    prediction = model_loaded([tensor_img1])[0]\n",
    "\n",
    "    \n",
    "\n",
    "    labels = [labels_dict[str(label.item())] + ': ' + \\\n",
    "            str(round(prediction[\"scores\"][idx].item(), 2)) \\\n",
    "            for idx, label in enumerate(prediction[\"labels\"])]\n",
    "\n",
    "    colors = [colors_dict[str(label.item())] \\\n",
    "              for idx, label in enumerate(prediction[\"labels\"])]\n",
    "\n",
    "    box = draw_bounding_boxes(pil_to_tensor(img1), # for original image case\n",
    "                            #(tensor_img1*256).to(dtype=torch.uint8), # for normalized image case (not fully identical)\n",
    "                            boxes=prediction['boxes'],\n",
    "                            labels=labels,\n",
    "                            colors=colors,\n",
    "                            width=3)\n",
    "    im = to_pil_image(box.detach())\n",
    "    im.show()\n",
    "\n",
    "    print(img_name, prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
